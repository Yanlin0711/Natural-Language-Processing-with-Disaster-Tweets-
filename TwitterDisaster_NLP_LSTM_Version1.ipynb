{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424c5384",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a783d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Dropout,LSTM,SimpleRNN,Embedding,Bidirectional,LSTM,GlobalMaxPool1D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1f8f3",
   "metadata": {},
   "source": [
    "# Import CSV files and clean missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522c9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('C:/Users/yanli/Downloads/train.csv')\n",
    "data_test = pd.read_csv('C:/Users/yanli/Downloads/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532d1b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3100c16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09370a1",
   "metadata": {},
   "source": [
    "From Line[3] and Line[4], we found out that there are missing values in both train and test datasets, which are in 'location' and 'keyword' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abbebc",
   "metadata": {},
   "source": [
    "Since these two features will not have influence on our prediction model, we will simply drop them. We will drop the feature 'id' too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee176d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(['id','keyword','location'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231b992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.drop(['id','keyword','location'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a8781",
   "metadata": {},
   "source": [
    "Train and test datasets after dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af26e82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
       "5  #RockyFire Update => California Hwy. 20 closed...       1\n",
       "6  #flood #disaster Heavy rain causes flash flood...       1\n",
       "7  I'm on top of the hill and I can see a fire in...       1\n",
       "8  There's an emergency evacuation happening now ...       1\n",
       "9  I'm afraid that the tornado is coming to our a...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c872486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 Just happened a terrible car crash\n",
       "1  Heard about #earthquake is different cities, s...\n",
       "2  there is a forest fire at spot pond, geese are...\n",
       "3           Apocalypse lighting. #Spokane #wildfires\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5                 We're shaking...It's an earthquake\n",
       "6  They'd probably still show more life than Arse...\n",
       "7                                  Hey! How are you?\n",
       "8                                   What a nice hat?\n",
       "9                                          Fuck off!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143d7c5",
   "metadata": {},
   "source": [
    "# Design Natural Language Processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9f9cd",
   "metadata": {},
   "source": [
    "## 1. Tokenizing the string\n",
    "## 2. Converting characters to lowercase\n",
    "## 3. Removing stop words and punctuations\n",
    "## 4. Stemming or lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe91dc9",
   "metadata": {},
   "source": [
    "Import NLP Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870d50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yanli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yanli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer,TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62a2a9",
   "metadata": {},
   "source": [
    "### Function that remove unnecessary characters such as URL, http and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13bdd8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveUnnecessaryChar(sentence):\n",
    "    for sentence1 in sentence:\n",
    "        sentence1 = str(sentence1)\n",
    "        sentence1 = sentence1.lower()\n",
    "        formatted_sent = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',sentence1) #  Remove hyperlinks\n",
    "        formatted_sent = formatted_sent.replace('{html}',\"\")\n",
    "        formatted_sent = re.sub(r'#','',formatted_sent) # Removed Hashtags\n",
    "        formatted_sent = re.sub(r'[0-9]','',formatted_sent) # Removes Numbers\n",
    "        formatted_sent = re.sub(r'@[A-Za-z]*','',formatted_sent) # Removed @ Tags\n",
    "        \n",
    "        sent.append(formatted_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3f5e1",
   "metadata": {},
   "source": [
    "### Function that tokenize sentences. The goal of this function is to sepearate the whole sentence apart and treat every word as an individual element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06ecea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenizeSentence(sentence):\n",
    "    tokenizer = TweetTokenizer(preserve_case = False,strip_handles = True,reduce_len=True)\n",
    "    for sentence in sent:\n",
    "        tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "        tokenized_sent.append(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87621e",
   "metadata": {},
   "source": [
    "### Function that find and delete stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da5366c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordsSentence(sent):\n",
    "    for sentence in sent:\n",
    "        formatted_words=[]\n",
    "        for word in sentence:\n",
    "            if word not in stopwords_eng and word not in string.punctuation and len(word)>2:\n",
    "                formatted_words.append(word)\n",
    "        formatted_sent.append(formatted_words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e071f8f",
   "metadata": {},
   "source": [
    "### Function that do lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0769225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeSentence(sent):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for sentence in sent:\n",
    "            lemma_words = []\n",
    "            for word in sentence:\n",
    "                lemma_word = lemma.lemmatize(word)\n",
    "                lemma_words.append(lemma_word)\n",
    "            lemma_sent.append(lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be222e",
   "metadata": {},
   "source": [
    "### Function that generate the final sentence for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8465ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalSentence(sentence1):\n",
    "    for sentence in sentence1:\n",
    "        sent = ' '.join([str(word) for word in sentence])\n",
    "        final_sentence_list.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda0c93",
   "metadata": {},
   "source": [
    "### Start Sanitizing 'text' column in the train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e9d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "RemoveUnnecessaryChar(data_train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3ca5c",
   "metadata": {},
   "source": [
    "Test first step: Unnecessary character removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efd06001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest fire near la ronge sask. canada\n"
     ]
    }
   ],
   "source": [
    "print(sent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c1804f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = []\n",
    "TokenizeSentence(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979bc74",
   "metadata": {},
   "source": [
    "Test the second step: Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "635cb650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f8fe9",
   "metadata": {},
   "source": [
    "Set stopwords and punctuations for the third step: Remove stop words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac9e00f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Stop Words :\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuations  :\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_eng = stopwords.words('english')\n",
    "print('English Stop Words :\\n')\n",
    "print(stopwords_eng)\n",
    "print('\\nPunctuations  :\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcb51a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sent = []\n",
    "stopwordsSentence(tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe258f",
   "metadata": {},
   "source": [
    "Test the third step: Remove stop words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99150c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest', 'fire', 'near', 'ronge', 'sask', 'canada']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_sent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55c56a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_sent = []\n",
    "lemmatizeSentence(formatted_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f511e23",
   "metadata": {},
   "source": [
    "Test the fourth step: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c5f54e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest', 'fire', 'near', 'ronge', 'sask', 'canada']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sent[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca2f4e",
   "metadata": {},
   "source": [
    "Generate the final sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebe98119",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentence_list = []\n",
    "finalSentence(lemma_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "742b11c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forest fire near ronge sask canada'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentence_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a31a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['FinalText'] = final_sentence_list\n",
    "data_train.head()\n",
    "data_train = data_train.drop(['text'],axis = 1) # Drop the 'text' column to get clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff142f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>FinalText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                          FinalText\n",
       "0       1           deed reason earthquake may allah forgive\n",
       "1       1                 forest fire near ronge sask canada\n",
       "2       1  resident asked shelter place notified officer ...\n",
       "3       1  people receive wildfire evacuation order calif...\n",
       "4       1  got sent photo ruby alaska smoke wildfire pour..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc197411",
   "metadata": {},
   "source": [
    "# Convert Finaltext into a numerical vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27c87143",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train['FinalText']\n",
    "y_train = data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ae9a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab6cac",
   "metadata": {},
   "source": [
    "### Import Tensorflow package to numerize our text array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a37cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e0aca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_array,y_train_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "378f6916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83eb488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  b'deed reason earthquake may allah forgive'\n",
      "Label:  1\n"
     ]
    }
   ],
   "source": [
    "for text,label in train_dataset.take(1):\n",
    "    print('Text: ',text.numpy())\n",
    "    print('Label: ',label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0364660",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 3000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dea3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cbf436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "\n",
    "\n",
    "#This layer will only be used in LSTM and GRU architectures for obtaining numerical vector representation of words. \n",
    "#For BERT we will use bert spcific vectorization technique.\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens = VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text,target: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0c28207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[UNK]', 'fire', 'like', 'get', 'im', 'new', 'one', 'people',\n",
       "       'time'], dtype='<U29')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = np.array(encoder.get_vocabulary())\n",
    "vocabulary[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eacbaf2",
   "metadata": {},
   "source": [
    "The encoder function can convert the texte array into numerical array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3958bef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text :tf.Tensor(b'deed reason earthquake may allah forgive', shape=(), dtype=string)\n",
      "Numeric Representaion :[5033  425  220   60 1424 4811]\n"
     ]
    }
   ],
   "source": [
    "print('Original Text :' +str(text))\n",
    "encoded_text = encoder(text).numpy()\n",
    "print('Numeric Representaion :' +str(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b4eb416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e67d7",
   "metadata": {},
   "source": [
    "# Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e04bc626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 16)          192000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 32)         4224      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 32)          0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                330       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,565\n",
      "Trainable params: 196,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(encoder)\n",
    "model.add(Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=16,mask_zero = True))\n",
    "model.add(Bidirectional(LSTM(16,return_sequences = True)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86601296",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',factor=0.25,patience=2,min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3972fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8bd2d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.6829 - accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.5459 - accuracy: 0.6692 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 4s 75ms/step - loss: 0.3309 - accuracy: 0.8675 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 5s 75ms/step - loss: 0.2401 - accuracy: 0.9100 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.1851 - accuracy: 0.9355 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,epochs = 5,callbacks = [reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac73d4d",
   "metadata": {},
   "source": [
    "# Prepare our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12229782",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "RemoveUnnecessaryChar(data_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0f08c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = []\n",
    "TokenizeSentence(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50014a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sent = []\n",
    "stopwordsSentence(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "953fd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_sent = []\n",
    "lemmatizeSentence(formatted_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4a804ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentence_list = []\n",
    "finalSentence(lemma_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0aa9f269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>earthquake safety los angeles safety fastener ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>storm worse last hurricane city others hardest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>green line derailment chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>meg issue hazardous weather outlook hwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>cityofcalgary activated municipal emergency pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                           happened terrible car crash\n",
       "1     heard earthquake different city stay safe ever...\n",
       "2     forest fire spot pond goose fleeing across str...\n",
       "3                  apocalypse lighting spokane wildfire\n",
       "4                    typhoon soudelor kill china taiwan\n",
       "...                                                 ...\n",
       "3258  earthquake safety los angeles safety fastener ...\n",
       "3259  storm worse last hurricane city others hardest...\n",
       "3260                      green line derailment chicago\n",
       "3261            meg issue hazardous weather outlook hwo\n",
       "3262  cityofcalgary activated municipal emergency pl...\n",
       "\n",
       "[3263 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['text'] = final_sentence_list\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d2154d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data_test['text']\n",
    "x_test_array = x_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dc96817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  b'happened terrible car crash'\n",
      "Text:  b'heard earthquake different city stay safe everyone'\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_array))\n",
    "for test_text in test_dataset.take(2):\n",
    "    print('Text: ', test_text.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2cfed4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d4efb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df1c60f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2798808],\n",
       "       [1.8644623],\n",
       "       [4.266029 ],\n",
       "       ...,\n",
       "       [1.4505811],\n",
       "       [2.8034046],\n",
       "       [1.9419159]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f54f8b",
   "metadata": {},
   "source": [
    "### Make 'Target' column: If prediction probability > 0, there is a disaster. Otherwise there is no disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4180e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in y_pred:\n",
    "    if i >= 0:\n",
    "        result.append(1)\n",
    "    else: \n",
    "        result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9bbbb19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('C:/Users/yanli/Downloads/sample_submission.csv')\n",
    "submission['target'] = result\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a858ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1953\n",
       "1    1310\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5fd4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('C:/Users/yanli/Downloads/NLPsubmission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f47c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
